{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca41fb2a-c7aa-47f6-aab1-bf043f0c4b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "#%pip install torch\n",
    "#importing libraries\n",
    "import dgl\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn # for link prediction\n",
    "\n",
    "import tqdm\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import itertools\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import os\n",
    "\n",
    "from numpy import array\n",
    "from numpy import split\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import csv\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574ec9f-6108-49a3-bce5-159f76740af1",
   "metadata": {},
   "source": [
    "# Dot Predictor Class for evaluating Link Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e7aaff-10a8-49c4-bf85-731536aef425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4dbf2-bf04-42e9-a8bc-d2172bcd0388",
   "metadata": {},
   "source": [
    "# Defining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f96f02b-6d19-4770-9598-78a0d3de4b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn import GATConv\n",
    "from dgl.nn import GraphConv\n",
    "from dgl.nn import GatedGraphConv\n",
    "\n",
    "\n",
    "#defining the Model\n",
    "class Model_SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model_SAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "\n",
    "    def forward(self, mfgs, x):\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()] \n",
    "        h = self.conv1(mfgs[0], (x, h_dst))  \n",
    "        h = F.relu(h)\n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]  \n",
    "        h = self.conv2(mfgs[1], (h, h_dst))  \n",
    "        return h\n",
    "\n",
    "#defining the Model\n",
    "class Model_GraphConv(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model_GraphConv, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats, allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, mfgs, x):\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()] \n",
    "        h = self.conv1(mfgs[0], (x, h_dst))  \n",
    "        h = F.relu(h)\n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]  \n",
    "        h = self.conv2(mfgs[1], (h, h_dst))  \n",
    "        return h\n",
    "\n",
    "\n",
    "class Model_GATConv(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model_GATConv, self).__init__()\n",
    "        self.conv1 = GATConv(in_feats, h_feats, num_heads = 1, feat_drop = 0.4, attn_drop = 0.2, allow_zero_in_degree=True)\n",
    "        self.conv2 = GATConv(h_feats, num_classes, num_heads = 1, feat_drop = 0.4, attn_drop = 0.2, allow_zero_in_degree=True)\n",
    "        \n",
    "        #@TODO\n",
    "        self.classify = nn.Sequential(nn.Linear(333 * 1, num_classes))\n",
    "        \n",
    "        #self.linear=nn.Linear(h_feats,num_classes)\n",
    "    \n",
    "    def forward(self, mfgs, x):\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()] \n",
    "        h = self.conv1(mfgs[0], (x, h_dst))  \n",
    "        h = F.relu(h)\n",
    "        h = h.mean(dim=1) \n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]  \n",
    "        h = self.conv2(mfgs[1], (h, h_dst))  \n",
    "        h = h.mean(dim=1) \n",
    "        return h\n",
    "    \n",
    "class Model_Gated(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model_Gated, self).__init__()\n",
    "        self.conv1 = GatedGraphConv(in_feats, h_feats, 2, 1)\n",
    "        self.conv2 = GatedGraphConv(h_feats, num_classes, 2, 1)\n",
    "        \n",
    "    def forward(self, mfgs, x):\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()] \n",
    "        h = self.conv1(mfgs[0], (x, h_dst), None)  \n",
    "        h = F.relu(h)\n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]  \n",
    "        h = self.conv2(mfgs[1], (h, h_dst), None)  \n",
    "        return h\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Link_GraphConv(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(Link_GraphConv, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats, allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(h_feats, h_feats, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "    \n",
    "class Link_GATConv(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(Link_GATConv, self).__init__()\n",
    "        self.conv1 = GATConv(in_feats, h_feats, num_heads = 1, feat_drop = 0.4, attn_drop = 0.2, allow_zero_in_degree=True)\n",
    "        self.conv2 = GATConv(h_feats, h_feats, num_heads = 1, feat_drop = 0.4, attn_drop = 0.2, allow_zero_in_degree=True)\n",
    "        \n",
    "        #@TODO\n",
    "        self.classify = nn.Sequential(nn.Linear(333 * 1, h_feats))\n",
    "        \n",
    "        #self.linear=nn.Linear(h_feats,num_classes)\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = h.mean(dim=1)\n",
    "        h = self.conv2(g, h)\n",
    "        h = h.mean(dim=1)\n",
    "        return h\n",
    "    \n",
    "def Model(alg, num_features, num_classes, device):\n",
    "  if (alg == \"SAGEConv\"):\n",
    "    model = Model_SAGE(num_features, 128, num_classes).to(device)\n",
    "  elif (alg == \"GraphConv\"):\n",
    "    model = Model_GraphConv(num_features, 128, num_classes).to(device)\n",
    "  elif (alg == \"GATConv\"):\n",
    "    model = Model_GATConv(num_features, 128, num_classes).to(device)\n",
    "  elif (alg == \"GatedGraphConv\"):\n",
    "    model = Model_Gated(num_features, 128, num_classes).to(device)\n",
    "  return model\n",
    "\n",
    "def Model_link(alg, num_features):\n",
    "    if (alg == \"SAGEConv\"):\n",
    "        model = GraphSAGE(num_features, 16)\n",
    "    elif (alg == \"GraphConv\"):\n",
    "        model = Link_GraphConv(num_features, 16)\n",
    "    elif (alg == \"GATConv\"):\n",
    "        model = Link_GATConv(num_features, 16)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df166344-4dbf-43e9-a971-88acffce6a23",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b83e5a4-051a-432b-8e76-f04e7429924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import CiteseerGraphDataset\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl.data import PubmedGraphDataset\n",
    "from dgl.data import CoauthorCSDataset\n",
    "from dgl.data import CoauthorPhysicsDataset\n",
    "\n",
    "def load_dataset(dataset):\n",
    "  if(dataset == 'Cora'):\n",
    "    return CoraGraphDataset()\n",
    "  if(dataset == 'Citeseer'):\n",
    "    return CiteseerGraphDataset()\n",
    "  if(dataset == 'Pubmed'):\n",
    "    return PubmedGraphDataset()\n",
    "  if(dataset == 'CoauthorCS'):\n",
    "    return CoauthorCSDataset()\n",
    "  if(dataset == 'Physics'):\n",
    "    return CoauthorPhysicsDataset()\n",
    "\n",
    "def overlay_edges(dataset, cos):\n",
    "  ###\n",
    "  # 1) Per grafi piccoli: \n",
    "  #    pairwise cosine similarity tra le feature iniziali (gia' disponibili dal dataset)\n",
    "  #    esempi di feature solo le keyword degli articoli, ecc.\n",
    "  # 2) Per grafi grandi:\n",
    "  #    pairwise cosine similarity ma tenendo solo quelli che superano una soglia\n",
    "  #    - ad esempio se la soglia e' 0.5 per ogni nodo n, avro' solo i vicini  (tra tutti)\n",
    "  #    per i quali c'e' una similarita' uguale o superiore a 0.5\n",
    "  #    Se uno n' non e' vicino di n per una soglia s1 non lo sara' anche per s2>s1\n",
    "  # - Abbiamo quindi \"matrici\" di similarita' tra nodi per 0.5, 0.6, 0.7, 0.8, 0.9\n",
    "  #  \n",
    "  ###\n",
    "\n",
    "\n",
    "  # 1) Caricare gli edge weights dal disco e immetterli come edge feature nel grafo\n",
    "  # \n",
    "\n",
    "  ##############################\n",
    "  cos = str(round(cos, 2)).translate(str.maketrans(\"\", \"\", string.punctuation)) #cos is a float 0.x, but I need the string \"0x\"\n",
    "  path = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/Overlay GNN/Overlay Edges/\" + dataset + \"/overlay_edges_\" + cos\n",
    "  with open(path, \"rb\") as fp:\n",
    "    overlay_edges = pickle.load(fp)\n",
    "  return (overlay_edges[0], overlay_edges[1])\n",
    "  ############################################################\n",
    "\n",
    "### Edges are removed from nodes that will have new edges added from the overlay\n",
    "## @TODOs: sarebbe utile non rimuovere random? ma quelli con similarita' minore?\n",
    "def remove(graph, seed_list):\n",
    "  n_overlay = {}\n",
    "  for seed_node in seed_list:\n",
    "    if (seed_node not in n_overlay):\n",
    "      n_overlay[seed_node] = 1\n",
    "    else:\n",
    "      n_overlay[seed_node] = n_overlay[seed_node] + 1\n",
    "  #defining an utility function inside remove\n",
    "  def remove_edges(graph, node, n_overlay_edges, n_removable_edges, out_edges):\n",
    "    if(n_removable_edges == 0):\n",
    "      return 0\n",
    "    remove_edges = []\n",
    "    while(n_overlay_edges > 0 and n_removable_edges > 0):\n",
    "      remove_edges.append(out_edges.pop(random.randrange(len(out_edges))))\n",
    "      n_overlay_edges -= 1\n",
    "      n_removable_edges -= 1\n",
    "    tensor_removed = []\n",
    "    for edge in remove_edges:\n",
    "        tensor_removed.append(graph.find_edges(edge))\n",
    "    graph = dgl.remove_edges(graph, remove_edges)\n",
    "    return len(remove_edges), graph, tensor_removed\n",
    "  #now back inside remove\n",
    "  adj_matrix = graph.adj(scipy_fmt='coo') #getting the adj_matrix in the scipy coo sparse matrix\n",
    "  total_removed = 0\n",
    "  tensor_total = []\n",
    "  n_overlay_after = {} #dictionary that helps us adding only a number of overlay edges equal to the number of removed edges\n",
    "  for key in n_overlay:\n",
    "        n_overlay_after[key] = 0\n",
    "  for node in n_overlay:\n",
    "    k = n_overlay[node]\n",
    "    out_edges = graph.out_edges(node)\n",
    "    n_removable = len(out_edges[0])\n",
    "    out_edges = graph.edge_ids(out_edges[0], out_edges[1]) \n",
    "    #now out_edges is a tensor(id_1, id_2, ... , id_k) where id_x is an edge id \n",
    "    removed, graph, tensor_removed = remove_edges(graph, node, k, n_removable, out_edges.tolist())\n",
    "    n_overlay_after[node] = removed\n",
    "    total_removed += removed\n",
    "    tensor_total.extend(tensor_removed)\n",
    "  return graph, total_removed, n_overlay_after, n_overlay, tensor_total\n",
    "\n",
    "\n",
    "def addEdges(graph, seeds, dests, dict_after, dict_before):\n",
    "    dict_remove = {} #for each node, it stores the number of edges to be removed from seeds and dests\n",
    "    for key in dict_before:\n",
    "        dict_remove[key] = dict_before[key] - dict_after[key]\n",
    "    indices_mask = []\n",
    "    for i in range(len(seeds)):\n",
    "        node = seeds[i]\n",
    "        if (dict_remove[node] > 0):\n",
    "            indices_mask.append(False)\n",
    "            dict_remove[node] -= 1\n",
    "        else:\n",
    "            indices_mask.append(True)\n",
    "    seeds = np.array(seeds) #change in numpy array required in order to perform masking\n",
    "    dests = np.array(dests)\n",
    "    seeds = seeds[indices_mask]\n",
    "    dests = dests[indices_mask]\n",
    "    return len(seeds.tolist()), seeds.tolist(), dests.tolist()\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]\n",
    "        \n",
    "def remove_negatives(train_neg_u, train_neg_v, seed_list, dest_list):\n",
    "    new_train_neg_u = []\n",
    "    new_train_neg_v = []\n",
    "    for i in range(len(train_neg_u)):\n",
    "        ns = train_neg_u[i]\n",
    "        nd = train_neg_v[i]\n",
    "        yes = True\n",
    "        for j in range(len(seed_list)):\n",
    "            s = seed_list[j]\n",
    "            d = dest_list[j]\n",
    "            if(s == ns and d == nd):\n",
    "                yes = False\n",
    "                break\n",
    "        if(yes):\n",
    "            new_train_neg_u.append(ns)\n",
    "            new_train_neg_v.append(nd)\n",
    "    return new_train_neg_u, new_train_neg_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac23064-a976-4b71-ba7e-54b33fefac5b",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98082d46-b6fb-4afd-b86b-c409d6eb97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(alg, dataset_str, graph, device, feat, labels, num_classes, num_features, train_nids, test_nids, valid_nids, total_removed, total_added, cos_sim, per_train, num_iterations, b_size, path_iter, path_avg, n_epoch):\n",
    "  #Initialize Model \n",
    "  model = Model(alg, num_features, num_classes, device)\n",
    "\n",
    "  \n",
    "  sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "\n",
    "  # @TODO: my_sampler=ImplementazioneMySampler\n",
    "\n",
    "\n",
    "  #Initialize the training dataloader with MultilayerNeighborSampler.\n",
    "  train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "      # The following arguments are specific to NodeDataLoader.\n",
    "      graph,              # The graph\n",
    "      train_nids,         # The node IDs to iterate over in minibatches\n",
    "      sampler,            # The neighbor sampler\n",
    "      device=device,      # Put the sampled MFGs on CPU or GPU\n",
    "      # The following arguments are inherited from PyTorch DataLoader.\n",
    "      batch_size=b_size,    # Batch size\n",
    "      shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "      drop_last=False,    # Whether to drop the last incomplete batch\n",
    "      #num_workers=0       # Number of sampler processes\n",
    "  )\n",
    "\n",
    "  #Initialize validation dataloader.\n",
    "  valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "      graph, valid_nids, sampler,\n",
    "      batch_size=b_size,\n",
    "      shuffle=False,\n",
    "      drop_last=False,\n",
    "      num_workers=0,\n",
    "      device=device\n",
    "  )\n",
    "  iterations = num_iterations\n",
    "  acc_array = []*iterations\n",
    "  while (iterations > 0):\n",
    "    #Initialize model and define the optimizer.\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    #Training loop.\n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(n_epoch):\n",
    "        model.train() #we put our model in training mode. In training we train with something we already know.\n",
    "        with tqdm.tqdm(train_dataloader) as tq: #this statement refers to the progress bar.\n",
    "            for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "                # feature copy from CPU to GPU takes place here\n",
    "                inputs = mfgs[0].srcdata['feat'] #we get the input features from the first layer.\n",
    "                labels = mfgs[-1].dstdata['label'] #and the labels from the last one. These are the ones that we'll use to compare with our predictions.\n",
    "                \n",
    "                predictions = model(mfgs, inputs) #getting predictions.\n",
    "                print(\"predictions.shape=\",predictions.shape)\n",
    "                print(\"labels.shape=\",labels.shape)\n",
    "\n",
    "                #print(predictions)\n",
    "                #exit()\n",
    "                #print(#######)\n",
    "                \n",
    "                #if(alg == \"GATConv\"):\n",
    "                #predictions = torch.argmax(predictions, dim=1)\n",
    "                \n",
    "                #for p in predictions:\n",
    "                #    print(p)\n",
    "                #print(predictions)  \n",
    "                \n",
    "                loss = F.cross_entropy(predictions, labels) #measuring loss. Between predictions and what we get.\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                ###########\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "                tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "\n",
    "        model.eval() #we switch to evaluation mode.\n",
    "\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "            for input_nodes, output_nodes, mfgs in tq:\n",
    "                inputs = mfgs[0].srcdata['feat']\n",
    "                labels.append(mfgs[-1].dstdata['label'].cpu().numpy())\n",
    "                predictions.append(model(mfgs, inputs).argmax(1).cpu().numpy())\n",
    "            predictions = np.concatenate(predictions)\n",
    "            labels = np.concatenate(labels)\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "            if (epoch == 99):\n",
    "              print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "            if best_accuracy < accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "    acc_array.append(round(accuracy, 3))\n",
    "    #refresh model for next iteration\n",
    "    model = Model(alg, num_features, num_classes, device)\n",
    "    iterations -= 1\n",
    "  \n",
    "  #now writing on the result iter file all the accuracies saved in acc_array\n",
    "  with open(path_iter, \"a\") as csvfile:\n",
    "    fieldnames = [\"Iteration\", \"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"Accuracy\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    for i in reversed(range(len(acc_array))):\n",
    "      writer.writerow({\"Iteration\": i, \"Algorithm\": alg, \"Dataset\": dataset_str, \"ThresholdSim\": str(round(cos_sim,2)), \"Training%\": str(round(per_train, 4)), \"NumAddedEdges\": total_added, \"NumRemovedEdges\": total_removed, \"Accuracy\": acc_array[i]})\n",
    "\n",
    "  \n",
    "  \n",
    "  with open(path_avg, \"a\") as csvfile:\n",
    "      fieldnames = [\"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"AvgAccuracy\"]\n",
    "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "      writer.writerow({\"Algorithm\": alg, \"Dataset\": dataset_str, \"ThresholdSim\": str(round(cos_sim,2)), \"Training%\": str(round(per_train, 4)), \"NumAddedEdges\": total_added, \"NumRemovedEdges\": total_removed, \"AvgAccuracy\": round(sum(acc_array)/len(acc_array), 3)})\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7abef3f-490c-4df7-9c03-79247cbb2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_link(alg, train_g, n_epoch, num_iterations, train_pos_g, train_neg_g, test_pos_g, test_neg_g, dataset_str, cos_sim, per_train, total_added, total_removed, path_iter, path_avg):\n",
    "    model = Model_link(alg, train_g.ndata['feat'].shape[1])\n",
    "    pred = DotPredictor()\n",
    "\n",
    "    def compute_loss(pos_score, neg_score):\n",
    "        scores = torch.cat([pos_score, neg_score])\n",
    "        labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "        return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "    def compute_auc(pos_score, neg_score):\n",
    "        scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "        labels = torch.cat(\n",
    "            [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "        return roc_auc_score(labels, scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------- training -------------------------------- #\n",
    "    acc_array = []*num_iterations\n",
    "    for i in range(num_iterations):\n",
    "        optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)\n",
    "        for e in range(n_epoch):\n",
    "            # forward\n",
    "            h = model(train_g, train_g.ndata['feat'])\n",
    "            pos_score = pred(train_pos_g, h)\n",
    "            neg_score = pred(train_neg_g, h)\n",
    "            loss = compute_loss(pos_score, neg_score)\n",
    "            #print('In epoch {}, loss: {}'.format(e, loss))\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ----------- check results ------------------------ #\n",
    "        with torch.no_grad():\n",
    "            pos_score = pred(test_pos_g, h)\n",
    "            neg_score = pred(test_neg_g, h)\n",
    "            auc = compute_auc(pos_score, neg_score)\n",
    "            print('AUC', auc)\n",
    "        acc_array.append(auc)\n",
    "        #refresh model for next iteration\n",
    "        model = Model_link(alg, train_g.ndata['feat'].shape[1])\n",
    "    #now writing on the result iter file all the accuracies saved in acc_array\n",
    "    with open(path_iter, \"a\") as csvfile:\n",
    "        fieldnames = [\"Iteration\", \"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"Accuracy\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        for i in range(len(acc_array)):\n",
    "          writer.writerow({\"Iteration\": i, \"Algorithm\": alg, \"Dataset\": dataset_str, \"ThresholdSim\": str(round(cos_sim,2)), \"Training%\": str(round(per_train, 4)), \"NumAddedEdges\": total_added, \"NumRemovedEdges\": total_removed, \"Accuracy\": acc_array[i]})\n",
    "\n",
    "\n",
    "\n",
    "    with open(path_avg, \"a\") as csvfile:\n",
    "        fieldnames = [\"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"AvgAccuracy\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({\"Algorithm\": alg, \"Dataset\": dataset_str, \"ThresholdSim\": str(round(cos_sim,2)), \"Training%\": str(round(per_train, 4)), \"NumAddedEdges\": total_added, \"NumRemovedEdges\": total_removed, \"AvgAccuracy\": round(sum(acc_array)/len(acc_array), 3)})\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908452f0-9e8d-415e-9ed3-647749f90dc2",
   "metadata": {},
   "source": [
    "# Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ea211f-9737-4c14-bd08-63b92b3f81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(dataset_str, alg, mode, training_values, cos_list, num_iterations, batch_size, path_iter, path_avg, n_epochs, bool_remove, device):\n",
    "    if (alg not in ['SAGEConv', 'GraphConv', 'GATConv', 'GatedGraphConv']):\n",
    "        print(\"Error: algorithm must be either SAGEConv, GraphConv, GatedGraphConv or GATConv, instead \" + alg + \" was given\")\n",
    "        return 1\n",
    "    if (dataset_str not in ['Cora', 'Citeseer', 'Pubmed', 'Physics', 'CoauthorCS']):\n",
    "        print(\"Error: dataset must be either Cora, Citeseer, Pubmed, Physics or CoauthorCS, instead \" + dataset_str + \" was given\")\n",
    "        return 1\n",
    "\n",
    "\n",
    "    dataset_base = load_dataset(dataset_str)\n",
    "    graph = dataset_base[0]\n",
    "\n",
    "    if(mode == \"Node\"):\n",
    "        num_classes = dataset_base.num_classes \n",
    "\n",
    "        feat = graph.ndata['feat'] #node features\n",
    "        labels = graph.ndata['label'] #ground truth labels (for node classification)\n",
    "        num_features = feat.shape[1]\n",
    "        n_nodes = graph.num_nodes() #number of nodes in the graph\n",
    "\n",
    "        #pre processing train, test and valid nids in order to use the same ones for different cosine similarity and make accurate comparisons\n",
    "        dict_train = {}\n",
    "        for training_value in training_values:\n",
    "            n_train = round(n_nodes*training_value) #effective number of train_nids\n",
    "            train_list = [i for i in range(n_nodes)]\n",
    "            train_list = random.sample(train_list, n_train) #sampling n_train values from train_list\n",
    "            train_mask = [True if i in train_list else False for i in range(n_nodes)]\n",
    "            train_nids = graph.nodes()[train_mask]\n",
    "            test_mask = np.logical_not(train_mask) > 0\n",
    "            test_nids = graph.nodes()[test_mask]\n",
    "            half = round(len(test_nids)/2)\n",
    "            val_mask = [True for _ in range(half)]\n",
    "            val_mask.extend([False for _ in range(len(test_nids) - half)])\n",
    "            valid_nids = test_nids[val_mask]\n",
    "            dict_train[training_value] = [train_nids, test_nids, valid_nids]\n",
    "\n",
    "        if(cos_list[0] < 0.1):\n",
    "            standard_done = False\n",
    "\n",
    "        for f_remove in bool_remove:\n",
    "            for cos in cos_list:\n",
    "                if(cos < 0.1 and standard_done == True):\n",
    "                    continue\n",
    "                elif(cos < 0.1 and standard_done == False):\n",
    "                    standard_done = True\n",
    "                #Loading Dataset\n",
    "                dataset = dataset_base\n",
    "                graph = dataset[0]\n",
    "\n",
    "                total_removed = 0\n",
    "                total_added = 0\n",
    "                if(cos > 0):\n",
    "                    #### these edges are NOT already present in the graph\n",
    "                    seed_list, dest_list = overlay_edges(dataset_str, cos)\n",
    "\n",
    "                    if (f_remove): #removing random neighbor edges\n",
    "                        graph, total_removed, n_overlay_after, dict_overlay, tensor_total = remove(graph, seed_list)\n",
    "                        #adding overlay edges\n",
    "                        total_added, seed_list, dest_list = addEdges(graph, seed_list, dest_list, n_overlay_after, dict_overlay)\n",
    "                    else:\n",
    "                        total_added = len(seed_list)\n",
    "                    graph.add_edges(torch.tensor(seed_list), torch.tensor(dest_list))\n",
    "                    if(alg in [\"GraphConv\", \"GATConv\"]):\n",
    "                        graph = dgl.add_self_loop(graph)\n",
    "                for training_value in training_values:\n",
    "                    train_nids, test_nids, valid_nids = dict_train[training_value]\n",
    "                    training(alg, dataset_str, graph, device, feat, labels, num_classes, num_features, train_nids, test_nids, valid_nids, total_removed, total_added, cos, training_value, num_iterations, batch_size, path_iter, path_avg, n_epochs)\n",
    "                \n",
    "    elif(mode == \"Link\"):\n",
    "        g = graph\n",
    "        # Split edge set for training and testing\n",
    "        u, v = g.edges()\n",
    "\n",
    "        eids = np.arange(g.number_of_edges())\n",
    "        eids = np.random.permutation(eids)\n",
    "        \n",
    "        \n",
    "        for percentage in training_values:\n",
    "            train_size = int(len(eids) * percentage)\n",
    "            test_size = g.number_of_edges() - train_size\n",
    "            test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "            train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "            # Find all negative edges and split them for training and testing\n",
    "            adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "            adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "            neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "            neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
    "            test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "            train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
    "\n",
    "            train_g = dgl.remove_edges(g, eids[:test_size])\n",
    "\n",
    "            train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "            train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "            test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "            test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
    "            if(cos_list[0] < 0.1):\n",
    "                standard_done = False\n",
    "            for f_remove in bool_remove:\n",
    "                for cos in cos_list:\n",
    "                    if(cos < 0.1 and standard_done == True):\n",
    "                        continue\n",
    "                    elif(cos < 0.1 and standard_done == False):\n",
    "                        standard_done = True\n",
    "                    total_added = 0\n",
    "                    total_removed = 0\n",
    "                    if(cos > 0.1):\n",
    "                        seed_list, dest_list = overlay_edges(dataset_str, cos)\n",
    "\n",
    "                        if (f_remove): #removing random neighbor edges\n",
    "                            graph, total_removed, n_overlay_after, dict_overlay, tensor_total = remove(graph, seed_list)\n",
    "                            remove_s = []\n",
    "                            remove_d = []\n",
    "                            for pair in tensor_total:\n",
    "                                remove_s.append(pair[0].item())\n",
    "                                remove_d.append(pair[1].item())\n",
    "                            new_train_pos_u, new_train_pos_v = remove_negatives(train_pos_u, train_pos_v, remove_s, remove_d)\n",
    "                            print(\"after remove negatives\")\n",
    "                            new_test_pos_u, new_test_pos_v = remove_negatives(test_pos_u, test_pos_v, remove_s, remove_d)\n",
    "                            train_pos_g = dgl.graph((new_train_pos_u, new_train_pos_v), num_nodes=g.number_of_nodes())\n",
    "                            test_pos_g = dgl.graph((new_test_pos_u, new_test_pos_v), num_nodes=g.number_of_nodes())\n",
    "                            #adding overlay edges\n",
    "                            total_added, seed_list, dest_list = addEdges(graph, seed_list, dest_list, n_overlay_after, dict_overlay)\n",
    "                        else:\n",
    "                            total_added = len(seed_list)\n",
    "                        graph.add_edges(torch.tensor(seed_list), torch.tensor(dest_list))\n",
    "                        new_train_neg_u, new_train_neg_v = remove_negatives(train_neg_u, train_neg_v, seed_list, dest_list)\n",
    "                        new_test_neg_u, new_test_neg_v = remove_negatives(test_neg_u, test_neg_v, seed_list, dest_list)\n",
    "                        train_neg_g = dgl.graph((new_train_neg_u, new_train_neg_v), num_nodes=g.number_of_nodes())\n",
    "                        test_neg_g = dgl.graph((new_test_neg_u, new_test_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "                    training_link(alg, train_g, n_epochs, num_iterations, train_pos_g, train_neg_g, test_pos_g, test_neg_g, dataset_str, cos, percentage, total_added, total_removed, path_iter, path_avg)\n",
    "                \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add84733-a74d-49a8-83bb-20b3063ff642",
   "metadata": {},
   "source": [
    "# General Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674e6cbc-b536-487d-83f7-ccb714a835c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general(dataset_list, mode, algorithm, cos_sim, training, step_cos, step_train, num_iterations, learning_rate, batch_size, path_iter, path_avg, no_overlay, remove_edges, n_epochs, device):\n",
    "\n",
    "#@TODO: \n",
    "# Learning rate\n",
    "# Adam: lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False, *, maximize=False\n",
    "\n",
    "  '''Safety Checks'''\n",
    "  if(len(dataset_list) == 0):\n",
    "    print(\"Error: Dataset List is Empty.\")\n",
    "    return 1\n",
    "  if(len(algorithm) == 0):\n",
    "    print(\"Error: Algorithm List is Empty\")\n",
    "    return 1\n",
    "  if(len(cos_sim) < 2):\n",
    "    print(\"Error: give low and high value for cosine similarity: [low, high]\")\n",
    "    return 1\n",
    "  if(len(training) < 2):\n",
    "    print(\"Error: give low and high value for training: [low, high]\")\n",
    "    return 1\n",
    "  if(cos_sim[0] > cos_sim[1]):\n",
    "    print(\"Error: cos_sim[0] must be less or equal than cos_sim[1]\")\n",
    "    return 1\n",
    "  if(training[0] > training[1]):\n",
    "    print(\"Error: training[0] must be less or equal than training[1]\")\n",
    "    return 1\n",
    "  if(device != \"cuda\" and device != \"cpu\"):\n",
    "    print(\"Error: device must be cuda or cpu\")\n",
    "    return 1\n",
    "\n",
    "  #Parsing Inputs:\n",
    "  datasets = [x for x in dataset_list]\n",
    "  algorithms = [x for x in algorithm]\n",
    "\n",
    "\n",
    "  train_low = training[0]\n",
    "  train_high = training[1]\n",
    "  training_values = np.arange(train_low, train_high+step_train, step_train)\n",
    "  \n",
    "  cos_low = cos_sim[0]\n",
    "  cos_high = cos_sim[1]\n",
    "  cos_list = np.arange(cos_low, cos_high+step_cos, step_cos)\n",
    "  if(no_overlay == 'y'):\n",
    "    cos_list = np.insert(cos_list, 0, 0.0)\n",
    "    \n",
    "  with open(path_iter, \"w\") as csvfile:\n",
    "    fieldnames = [\"Iteration\", \"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"Accuracy\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "  with open(path_avg, \"w\") as csvfile:\n",
    "    fieldnames = [\"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"AvgAccuracy\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "  if(remove_edges == 'y'):\n",
    "    bool_remove = [True]\n",
    "  elif(remove_edges == 'b'):\n",
    "    bool_remove = [True, False]\n",
    "  else:\n",
    "    bool_remove = [False]\n",
    "    \n",
    "  for dataset in datasets:\n",
    "      for alg in algorithms:\n",
    "        execution(dataset, alg, mode, training_values, cos_list, num_iterations, batch_size, path_iter, path_avg, n_epochs, bool_remove, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292ef89-dafc-4cf2-8059-a9fea309a051",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3315d5f6-ae82-45c4-b862-a40faca30a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path = '/home/studio-lab-user/sagemaker-studiolab-notebooks/Git/generalization_overlay/config_sage.ini'):\n",
    "  parser = configparser.ConfigParser()\n",
    "  parser.read(path)\n",
    "\n",
    "  dataset_list = [x for x in [\"Cora\", \"Citeseer\", \"Pubmed\", \"Physics\", \"CoauthorCS\"] if parser[\"DatasetList\"][x] == \"y\"]\n",
    "  alg_list = [x for x in [\"SAGEConv\", \"GraphConv\", \"GATConv\", \"GatedGraphConv\"] if parser[\"AlgList\"][x] == \"y\"]\n",
    "  cos_sim = [float(parser[\"RangeValues\"][\"CosLow\"]), float(parser[\"RangeValues\"][\"CosHigh\"])]\n",
    "  train_sim = [float(parser[\"RangeValues\"][\"TrainLow\"]), float(parser[\"RangeValues\"][\"TrainHigh\"])]\n",
    "  step_cos = float(parser[\"RangeValues\"][\"StepCos\"])\n",
    "  step_train = float(parser[\"RangeValues\"][\"StepTrain\"])\n",
    "\n",
    "  mode = parser[\"TrainingValues\"][\"Mode\"]\n",
    "  no_overlay = parser[\"TrainingValues\"][\"NoOverlay\"]\n",
    "  remove_edges = parser[\"TrainingValues\"][\"RemoveEdges\"]\n",
    "  n_epochs = int(parser[\"TrainingValues\"][\"NumEpochs\"])\n",
    "  num_iterations = int(parser[\"TrainingValues\"][\"NumIterations\"])\n",
    "  learning_rate = parser[\"TrainingValues\"][\"LearningRate\"]\n",
    "  batch_size = int(parser[\"TrainingValues\"][\"batch_size\"])\n",
    "  device = parser[\"TrainingValues\"][\"device\"]\n",
    "\n",
    "  path_iter = parser[\"Paths\"][\"SingleIteration\"]\n",
    "  path_avg = parser[\"Paths\"][\"Avg\"]\n",
    "\n",
    "  general(dataset_list, mode, alg_list, cos_sim, train_sim, step_cos, step_train, num_iterations, learning_rate, batch_size, path_iter, path_avg, no_overlay, remove_edges, n_epochs, device)\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b5b05e6-b4ea-4492-af3f-717774b446fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "  9%|▉         | 1/11 [00:00<00:08,  1.15it/s, loss=1.689, acc=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      " 18%|█▊        | 2/11 [00:01<00:06,  1.41it/s, loss=0.897, acc=0.725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      " 27%|██▋       | 3/11 [00:02<00:05,  1.55it/s, loss=0.580, acc=0.839]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      " 36%|███▋      | 4/11 [00:02<00:04,  1.63it/s, loss=0.421, acc=0.886]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      " 45%|████▌     | 5/11 [00:03<00:03,  1.68it/s, loss=0.317, acc=0.920]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      " 55%|█████▍    | 6/11 [00:03<00:02,  1.71it/s, loss=0.318, acc=0.911]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      " 64%|██████▎   | 7/11 [00:04<00:02,  1.72it/s, loss=0.235, acc=0.937]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      " 73%|███████▎  | 8/11 [00:04<00:01,  1.73it/s, loss=0.243, acc=0.939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      " 82%|████████▏ | 9/11 [00:05<00:01,  1.74it/s, loss=0.223, acc=0.938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([1024, 5])\n",
      "labels.shape= torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      " 91%|█████████ | 10/11 [00:06<00:00,  1.74it/s, loss=0.214, acc=0.926]/home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████| 11/11 [00:06<00:00,  1.79it/s, loss=0.124, acc=0.981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape= torch.Size([108, 5])\n",
      "labels.shape= torch.Size([108])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  4.59it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:300] . unexpected pos 4310144 vs 4310096",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_614/3329277088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/studio-lab-user/sagemaker-studiolab-notebooks/Git/generalization_overlay/config_link.ini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_614/786535014.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mpath_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Paths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mgeneral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malg_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_cos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_overlay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_614/1421842817.py\u001b[0m in \u001b[0;36mgeneral\u001b[0;34m(dataset_list, mode, algorithm, cos_sim, training, step_cos, step_train, num_iterations, learning_rate, batch_size, path_iter, path_avg, no_overlay, remove_edges, n_epochs, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0malg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexecution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_remove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_614/789860505.py\u001b[0m in \u001b[0;36mexecution\u001b[0;34m(dataset_str, alg, mode, training_values, cos_list, num_iterations, batch_size, path_iter, path_avg, n_epochs, bool_remove, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtraining_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mtrain_nids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_nids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_nids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_nids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_nids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_removed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_added\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Link\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_614/3131348215.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(alg, dataset_str, graph, device, feat, labels, num_classes, num_features, train_nids, test_nids, valid_nids, total_removed, total_added, cos_sim, per_train, num_iterations, b_size, path_iter, path_avg, n_epoch)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0macc_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m#refresh model for next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:300] . unexpected pos 4310144 vs 4310096"
     ]
    }
   ],
   "source": [
    "main('/home/studio-lab-user/sagemaker-studiolab-notebooks/Git/generalization_overlay/config_link.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b79a38-43bd-40fd-b3d0-3042f49e19d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGL_py36_pytorch1.2:Python",
   "language": "python",
   "name": "conda-env-DGL_py36_pytorch1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
