{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca41fb2a-c7aa-47f6-aab1-bf043f0c4b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/DGL_py36_pytorch1.2/lib/python3.9/site-packages (from torch) (4.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "#importing libraries\n",
    "import dgl\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "import os\n",
    "\n",
    "from numpy import array\n",
    "from numpy import split\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import csv\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4dbf2-bf04-42e9-a8bc-d2172bcd0388",
   "metadata": {},
   "source": [
    "# Defining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f96f02b-6d19-4770-9598-78a0d3de4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn import GATConv\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "\n",
    "#defining the Model\n",
    "class Model_SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model_SAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "\n",
    "    def forward(self, mfgs, x):\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()] \n",
    "        h = self.conv1(mfgs[0], (x, h_dst))  \n",
    "        h = F.relu(h)\n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]  \n",
    "        h = self.conv2(mfgs[1], (h, h_dst))  \n",
    "        return h\n",
    "\n",
    "#defining the Model\n",
    "class Model_GraphConv(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model_GraphConv, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, mfgs, x):\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()] \n",
    "        h = self.conv1(mfgs[0], (x, h_dst))  \n",
    "        h = F.relu(h)\n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]  \n",
    "        h = self.conv2(mfgs[1], (h, h_dst))  \n",
    "        return h\n",
    "\n",
    "\n",
    "#@TODO: implement Model_GATConv\n",
    "\n",
    "def Model(alg, num_features, num_classes, device):\n",
    "  if (alg == \"SAGEConv\"):\n",
    "    model = Model_SAGE(num_features, 128, num_classes).to(device)\n",
    "  elif (alg == \"GraphConv\"):\n",
    "    model = Model_GraphConv(num_features, 128, num_classes).to(device)\n",
    "  elif (alg == \"GATConv\"):\n",
    "    model = Model_GATConv(num_features, 128, num_classes).to(device)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df166344-4dbf-43e9-a971-88acffce6a23",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b83e5a4-051a-432b-8e76-f04e7429924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import CiteseerGraphDataset\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl.data import PubmedGraphDataset\n",
    "from dgl.data import CoauthorCSDataset\n",
    "from dgl.data import CoauthorPhysicsDataset\n",
    "\n",
    "def load_dataset(dataset):\n",
    "  if(dataset == 'Cora'):\n",
    "    return CoraGraphDataset()\n",
    "  if(dataset == 'Citeseer'):\n",
    "    return CiteseerGraphDataset()\n",
    "  if(dataset == 'Pubmed'):\n",
    "    return PubmedGraphDataset()\n",
    "  if(dataset == 'CoauthorCS'):\n",
    "    return CoauthorCSDataset()\n",
    "  if(dataset == 'Physics'):\n",
    "    return CoauthorPhysicsDataset()\n",
    "\n",
    "def overlay_edges(dataset, cos):\n",
    "  ###\n",
    "  # 1) Per grafi piccoli: \n",
    "  #    pairwise cosine similarity tra le feature iniziali (gia' disponibili dal dataset)\n",
    "  #    esempi di feature solo le keyword degli articoli, ecc.\n",
    "  # 2) Per grafi grandi:\n",
    "  #    pairwise cosine similarity ma tenendo solo quelli che superano una soglia\n",
    "  #    - ad esempio se la soglia e' 0.5 per ogni nodo n, avro' solo i vicini  (tra tutti)\n",
    "  #    per i quali c'e' una similarita' uguale o superiore a 0.5\n",
    "  #    Se uno n' non e' vicino di n per una soglia s1 non lo sara' anche per s2>s1\n",
    "  # - Abbiamo quindi \"matrici\" di similarita' tra nodi per 0.5, 0.6, 0.7, 0.8, 0.9\n",
    "  #  \n",
    "  ###\n",
    "\n",
    "\n",
    "  # 1) Caricare gli edge weights dal disco e immetterli come edge feature nel grafo\n",
    "  # \n",
    "\n",
    "  ##############################\n",
    "  cos = str(round(cos, 2)).translate(str.maketrans(\"\", \"\", string.punctuation)) #cos is a float 0.x, but I need the string \"0x\"\n",
    "  path = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/Overlay GNN/Overlay Edges/\" + dataset + \"/overlay_edges_\" + cos\n",
    "  with open(path, \"rb\") as fp:\n",
    "    overlay_edges = pickle.load(fp)\n",
    "  return (overlay_edges[0], overlay_edges[1])\n",
    "  ############################################################\n",
    "\n",
    "### Edges are removed from nodes that will have new edges added from the overlay\n",
    "## @TODOs: sarebbe utile non rimuovere random? ma quelli con similarita' minore?\n",
    "def remove(graph, seed_list):\n",
    "  n_overlay = {}\n",
    "  for seed_node in seed_list:\n",
    "    if (seed_node not in n_overlay):\n",
    "      n_overlay[seed_node] = 1\n",
    "    else:\n",
    "      n_overlay[seed_node] = n_overlay[seed_node] + 1\n",
    "  #defining an utility function inside remove\n",
    "  def remove_edges(node, n_overlay_edges, n_removable_edges, out_edges):\n",
    "    if(n_removable_edges == 0):\n",
    "      return 0\n",
    "    remove_edges = []\n",
    "    while(n_overlay_edges > 0 and n_removable_edges > 0):\n",
    "      remove_edges.append(out_edges.pop(random.randrange(len(out_edges))))\n",
    "      n_overlay_edges -= 1\n",
    "      n_removable_edges -= 1\n",
    "    dgl.remove_edges(graph, remove_edges)\n",
    "    return len(remove_edges)\n",
    "  #now back inside remove\n",
    "  adj_matrix = graph.adj(scipy_fmt='coo') #getting the adj_matrix in the scipy coo sparse matrix\n",
    "  total_removed = 0\n",
    "  for node in n_overlay:\n",
    "    k = n_overlay[node]\n",
    "    out_edges = graph.out_edges(node)\n",
    "    n_removable = len(out_edges[0])\n",
    "    out_edges = graph.edge_ids(out_edges[0], out_edges[1]) \n",
    "    #now out_edges is a tensor(id_1, id_2, ... , id_k) where id_x is an edge id \n",
    "    removed = remove_edges(node, k, n_removable, out_edges.tolist()) \n",
    "    total_removed += removed\n",
    "  return graph, total_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac23064-a976-4b71-ba7e-54b33fefac5b",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98082d46-b6fb-4afd-b86b-c409d6eb97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(alg, dataset_str, graph, device, feat, labels, num_classes, num_features, train_nids, test_nids, valid_nids, total_removed, total_added, cos_sim, per_train, num_iterations, b_size, path_iter, path_avg):\n",
    "  #Initialize Model \n",
    "  model = Model(alg, num_features, num_classes, device)\n",
    "\n",
    "  \n",
    "  sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "\n",
    "  # @TODO: my_sampler=ImplementazioneMySampler\n",
    "\n",
    "\n",
    "  #Initialize the training dataloader with MultilayerNeighborSampler.\n",
    "  train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "      # The following arguments are specific to NodeDataLoader.\n",
    "      graph,              # The graph\n",
    "      train_nids,         # The node IDs to iterate over in minibatches\n",
    "      sampler,            # The neighbor sampler\n",
    "      device=device,      # Put the sampled MFGs on CPU or GPU\n",
    "      # The following arguments are inherited from PyTorch DataLoader.\n",
    "      batch_size=b_size,    # Batch size\n",
    "      shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "      drop_last=False,    # Whether to drop the last incomplete batch\n",
    "      #num_workers=0       # Number of sampler processes\n",
    "  )\n",
    "\n",
    "  #Initialize validation dataloader.\n",
    "  valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "      graph, valid_nids, sampler,\n",
    "      batch_size=b_size,\n",
    "      shuffle=False,\n",
    "      drop_last=False,\n",
    "      num_workers=0,\n",
    "      device=device\n",
    "  )\n",
    "  iterations = num_iterations\n",
    "  acc_array = []*iterations\n",
    "  while (iterations > 0):\n",
    "    #Initialize model and define the optimizer.\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    #Training loop.\n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(100):\n",
    "        model.train() #we put our model in training mode. In training we train with something we already know.\n",
    "        with tqdm.tqdm(train_dataloader) as tq: #this statement refers to the progress bar.\n",
    "        \n",
    "            for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "                # feature copy from CPU to GPU takes place here\n",
    "                inputs = mfgs[0].srcdata['feat'] #we get the input features from the first layer.\n",
    "                labels = mfgs[-1].dstdata['label'] #and the labels from the last one. These are the ones that we'll use to compare with our predictions.\n",
    "                predictions = model(mfgs, inputs) #getting predictions.\n",
    "                loss = F.cross_entropy(predictions, labels) #measuring loss. Between predictions and what we get.\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                ###########\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "                tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "\n",
    "        model.eval() #we switch to evaluation mode.\n",
    "\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "            for input_nodes, output_nodes, mfgs in tq:\n",
    "                inputs = mfgs[0].srcdata['feat']\n",
    "                labels.append(mfgs[-1].dstdata['label'].cpu().numpy())\n",
    "                predictions.append(model(mfgs, inputs).argmax(1).cpu().numpy())\n",
    "            predictions = np.concatenate(predictions)\n",
    "            labels = np.concatenate(labels)\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "            if (epoch == 99):\n",
    "              print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "            if best_accuracy < accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "    acc_array.append(round(accuracy, 3))\n",
    "    #refresh model for next iteration\n",
    "    model = Model(alg, num_features, num_classes, device)\n",
    "    iterations -= 1\n",
    "  \n",
    "  #now writing on the result iter file all the accuracies saved in acc_array\n",
    "  with open(path_iter, \"a\") as csvfile:\n",
    "    fieldnames = [\"Iteration\", \"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"Accuracy\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    for i in reversed(range(len(acc_array))):\n",
    "      writer.writerow({\"Iteration\": i, \"Algorithm\": alg, \"Dataset\": dataset_str, \"ThresholdSim\": cos_sim, \"Training%\": str(round(per_train, 4)), \"NumAddedEdges\": total_added, \"NumRemovedEdges\": total_removed, \"Accuracy\": acc_array[i]})\n",
    "\n",
    "  \n",
    "  \n",
    "  with open(path_avg, \"a\") as csvfile:\n",
    "      fieldnames = [\"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"AvgAccuracy\"]\n",
    "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "      writer.writerow({\"Algorithm\": alg, \"Dataset\": dataset_str, \"ThresholdSim\": cos_sim, \"Training%\": str(round(per_train, 4)), \"NumAddedEdges\": total_added, \"NumRemovedEdges\": total_removed, \"AvgAccuracy\": round(sum(acc_array)/len(acc_array), 3)})\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908452f0-9e8d-415e-9ed3-647749f90dc2",
   "metadata": {},
   "source": [
    "# Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6ea211f-9737-4c14-bd08-63b92b3f81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(dataset_str, alg, training_values, cos_list, f_remove, num_iterations, batch_size, path_iter, path_avg, device):\n",
    "  if (alg not in ['SAGEConv', 'GraphConv', 'GATConv']):\n",
    "    print(\"Error: algorithm must be either SAGEConv, GraphConv or GATConv, instead \" + alg + \" was given\")\n",
    "    return 1\n",
    "  if (dataset_str not in ['Cora', 'Citeseer', 'Pubmed', 'Physics', 'CoauthorCS']):\n",
    "    print(\"Error: dataset must be either Cora, Citeseer, Pubmed, Physics or CoauthorCS, instead \" + dataset_str + \" was given\")\n",
    "    return 1\n",
    "\n",
    "\n",
    "  for cos in cos_list:\n",
    "    #Loading Dataset\n",
    "    dataset = load_dataset(dataset_str)\n",
    "    graph = dataset[0]\n",
    "    num_classes = dataset.num_classes #they are 6\n",
    "    \n",
    "    feat = graph.ndata['feat'] #node features\n",
    "    labels = graph.ndata['label'] #ground truth labels (for node classification)\n",
    "    num_features = feat.shape[1]\n",
    "    n_nodes = graph.num_nodes() #number of nodes in the graph\n",
    "\n",
    "    total_removed = 0\n",
    "    total_added = 0\n",
    "    if(cos > 0):\n",
    "        #### these edges are NOT already present in the graph\n",
    "        seed_list, dest_list = overlay_edges(dataset_str, cos)\n",
    "\n",
    "        if (f_remove): #removing random neighbor edges\n",
    "          graph, total_removed = remove(graph, seed_list)\n",
    "        #adding overlay edges\n",
    "        total_added = len(seed_list)\n",
    "        graph.add_edges(torch.tensor(seed_list), torch.tensor(dest_list)) \n",
    "    for training_value in training_values:\n",
    "      n_train = round(n_nodes*training_value) #effective number of train_nids\n",
    "      train_list = [i for i in range(n_nodes)]\n",
    "      train_list = random.sample(train_list, n_train) #sampling n_train values from train_list\n",
    "      train_mask = [True if i in train_list else False for i in range(n_nodes)]\n",
    "      train_nids = graph.nodes()[train_mask]\n",
    "\n",
    "      test_mask = np.logical_not(train_mask) > 0\n",
    "      test_nids = graph.nodes()[test_mask]\n",
    "\n",
    "      half = round(len(test_nids)/2)\n",
    "      val_mask = [True for _ in range(half)]\n",
    "      val_mask.extend([False for _ in range(len(test_nids) - half)])\n",
    "      valid_nids = test_nids[val_mask]\n",
    "      training(alg, dataset_str, graph, device, feat, labels, num_classes, num_features, train_nids, test_nids, valid_nids, total_removed, total_added, cos, training_value, num_iterations, batch_size, path_iter, path_avg)\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add84733-a74d-49a8-83bb-20b3063ff642",
   "metadata": {},
   "source": [
    "# General Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674e6cbc-b536-487d-83f7-ccb714a835c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general(dataset_list, algorithm, cos_sim, training, step_cos, step_train, num_iterations, learning_rate, batch_size, path_iter, path_avg, no_overlay, device):\n",
    "\n",
    "#@TODO: \n",
    "# Learning rate\n",
    "# batch_size\n",
    "# Adam: lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False, *, maximize=False\n",
    "\n",
    "  '''Safety Checks'''\n",
    "  if(len(dataset_list) == 0):\n",
    "    print(\"Error: Dataset List is Empty.\")\n",
    "    return 1\n",
    "  if(len(algorithm) == 0):\n",
    "    print(\"Error: Algorithm List is Empty\")\n",
    "    return 1\n",
    "  if(len(cos_sim) < 2):\n",
    "    print(\"Error: give low and high value for cosine similarity: [low, high]\")\n",
    "    return 1\n",
    "  if(len(training) < 2):\n",
    "    print(\"Error: give low and high value for training: [low, high]\")\n",
    "    return 1\n",
    "  if(cos_sim[0] > cos_sim[1]):\n",
    "    print(\"Error: cos_sim[0] must be less or equal than cos_sim[1]\")\n",
    "    return 1\n",
    "  if(training[0] > training[1]):\n",
    "    print(\"Error: training[0] must be less or equal than training[1]\")\n",
    "    return 1\n",
    "  if(device != \"cuda\" and device != \"cpu\"):\n",
    "    print(\"Error: device must be cuda or cpu\")\n",
    "    return 1\n",
    "\n",
    "  #Parsing Inputs:\n",
    "  datasets = [x for x in dataset_list]\n",
    "  algorithms = [x for x in algorithm]\n",
    "\n",
    "\n",
    "  train_low = training[0]\n",
    "  train_high = training[1]\n",
    "  training_values = np.arange(train_low, train_high+step_train, step_train)\n",
    "  \n",
    "  cos_low = cos_sim[0]\n",
    "  cos_high = cos_sim[1]\n",
    "  cos_list = np.arange(cos_low, cos_high+step_cos, step_cos)\n",
    "  if(no_overlay == 'y'):\n",
    "    cos_list = np.insert(cos_list, 0, 0.0)\n",
    "    \n",
    "  with open(path_iter, \"w\") as csvfile:\n",
    "    fieldnames = [\"Iteration\", \"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"Accuracy\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "  with open(path_avg, \"w\") as csvfile:\n",
    "    fieldnames = [\"Algorithm\", \"Dataset\", \"ThresholdSim\", \"Training%\", \"NumAddedEdges\", \"NumRemovedEdges\", \"AvgAccuracy\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    " \n",
    "  for dataset in datasets:\n",
    "    for alg in algorithms:\n",
    "      for f_remove_edges in [True, False]:\n",
    "        execution(dataset, alg, training_values, cos_list, f_remove_edges, num_iterations, batch_size, path_iter, path_avg, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292ef89-dafc-4cf2-8059-a9fea309a051",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3315d5f6-ae82-45c4-b862-a40faca30a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path = '/home/studio-lab-user/sagemaker-studiolab-notebooks/Git/generalization_overlay/config_sage.ini'):\n",
    "  parser = configparser.ConfigParser()\n",
    "  parser.read(path)\n",
    "\n",
    "  dataset_list = [x for x in [\"Cora\", \"Citeseer\", \"Pubmed\", \"Physics\", \"CoauthorCS\"] if parser[\"DatasetList\"][x] == \"y\"]\n",
    "  alg_list = [x for x in [\"SAGEConv\", \"GraphConv\", \"GATConv\"] if parser[\"AlgList\"][x] == \"y\"]\n",
    "  cos_sim = [float(parser[\"RangeValues\"][\"CosLow\"]), float(parser[\"RangeValues\"][\"CosHigh\"])]\n",
    "  train_sim = [float(parser[\"RangeValues\"][\"TrainLow\"]), float(parser[\"RangeValues\"][\"TrainHigh\"])]\n",
    "  step_cos = float(parser[\"RangeValues\"][\"StepCos\"])\n",
    "  step_train = float(parser[\"RangeValues\"][\"StepTrain\"])\n",
    "\n",
    "  no_overlay = parser[\"TrainingValues\"][\"NoOverlay\"]\n",
    "  num_iterations = int(parser[\"TrainingValues\"][\"NumIterations\"])\n",
    "  learning_rate = parser[\"TrainingValues\"][\"LearningRate\"]\n",
    "  batch_size = int(parser[\"TrainingValues\"][\"batch_size\"])\n",
    "  device = parser[\"TrainingValues\"][\"device\"]\n",
    "\n",
    "  path_iter = parser[\"Paths\"][\"SingleIteration\"]\n",
    "  path_avg = parser[\"Paths\"][\"Avg\"]\n",
    "\n",
    "  general(dataset_list, alg_list, cos_sim, train_sim, step_cos, step_train, num_iterations, learning_rate, batch_size, path_iter, path_avg, no_overlay, device)\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b05e6-b4ea-4492-af3f-717774b446fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e124bb-b650-402f-9a0e-158e52b78ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGL_py36_pytorch1.2:Python",
   "language": "python",
   "name": "conda-env-DGL_py36_pytorch1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
